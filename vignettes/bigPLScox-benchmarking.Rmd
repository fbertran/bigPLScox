---
title: "Benchmarking bigPLScox"
shorttitle: "Benchmarking bigPLScox"
author:
- name: "Frédéric Bertrand"
  affiliation:
  - Cedric, Cnam, Paris
  email: frederic.bertrand@lecnam.net
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Benchmarking bigPLScox}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "figures/benchmarking-",
  fig.width = 7,
  fig.height = 5,
  dpi = 150,
  message = FALSE,
  warning = FALSE
)
```

# Motivation

High-dimensional survival datasets can be computationally demanding. **bigPLScox**
implements algorithms that scale to large numbers of predictors and observations
via component-based models, sparse penalties, and stochastic gradient descent
routines. This vignette demonstrates how to benchmark the package against
baseline approaches using the [`bench`](https://bench.r-lib.org) package.

We focus on simulated data to illustrate reproducible comparisons between the
classical `coxgpls()` solver, its big-memory counterparts, and the
`survival::coxph()` implementation.

# Setup

The examples below require a recent version of **bench** together with
**survival**.

```{r packages}
library(bigPLScox)
library(survival)
library(bench)
```

The helper `dataCox()` simulates survival outcomes with right censoring. We work
with a moderately sized problem here, but larger values for `n` and `p` can be
used to stress test performance.

```{r simulate-data}
set.seed(2024)
sim_design <- dataCox(
  n = 2000,
  lambda = 2,
  rho = 1.5,
  x = matrix(rnorm(2000 * 50), ncol = 50),
  beta = c(1, 3, rep(0, 48)),
  cens.rate = 5
)

cox_data <- list(
  x = as.matrix(sim_design[, -(1:3)]),
  time = sim_design$time,
  status = sim_design$status
)
```

# Running the benchmark

We compare the classical Cox proportional hazards model with `coxgpls()`. The
`bench::mark()` helper executes the estimators multiple times and records timing
statistics alongside memory usage information.

```{r run-benchmarks}
bench_res <- bench::mark(
  bigPLScox = coxgpls(
    cox_data$x,
    cox_data$time,
    cox_data$status,
    ncomp = 5,
    ind.block.x = c(3, 10)
  ),
  survival = coxph(Surv(cox_data$time, cox_data$status) ~ cox_data$x, ties = "breslow"),
  iterations = 100,
  check = FALSE
)
bench_res
```

The resulting tibble reports elapsed time, memory allocations, and garbage
collection statistics for each estimator. The `itr/sec` column is often the most
useful indicator when comparing multiple implementations.

# Visualising the results

`bench` provides ggplot-based helpers to visualise the distributions of elapsed
and memory usage.

```{r bench-plot}
plot(bench_res, type = "jitter")
```

Additional geometries, such as ridge plots, are available via
`autoplot(bench_res, type = "ridge")`.

# Exporting benchmark results

Use the function `write.csv()` to store the benchmarking table as part of a
reproducible pipeline. For larger studies consider varying the number of latent
components, sparsity constraints, or the dataset dimensions.

```{r export-benchmark, eval = FALSE}
if (!dir.exists("inst/benchmarks/results")) {
  dir.create("inst/benchmarks/results", recursive = TRUE)
}
write.csv(bench_res, file = "inst/benchmarks/results/benchmarking-demo.csv", row.names = FALSE)
```

# Reusing the benchmarking scripts

The package also ships with standalone scripts under `inst/benchmarks/` that
mirror this vignette while exposing additional configuration points. Run them
from the repository root as:

```bash
Rscript inst/benchmarks/cox-benchmark.R
Rscript inst/benchmarks/benchmark_bigPLScox.R
Rscript inst/benchmarks/cox_pls_benchmark.R
```

Each script accepts environment variables to adjust the problem size and stores
results under `inst/benchmarks/results/` with time-stamped filenames.
