% Generated by hand to document arguments for bigSurvSGD.na.omit
\name{bigSurvSGD.na.omit}
\alias{bigSurvSGD.na.omit}
\title{Fit Survival Models with Stochastic Gradient Descent}
\description{
Performs stochastic gradient descent optimisation for large-scale survival
models after removing observations with missing values.
}
\usage{
bigSurvSGD.na.omit(
  formula,
  data,
  norm.method,
  features.mean,
  features.sd,
  opt.method,
  beta.init,
  beta.type,
  lr.const,
  lr.tau,
  strata.size,
  batch.size,
  num.epoch,
  b1,
  b2,
  eps,
  inference.method,
  num.boot,
  num.epoch.boot,
  boot.method,
  lr.const.boot,
  lr.tau.boot,
  num.sample.strata,
  sig.level,
  beta0,
  alpha,
  lambda,
  nlambda,
  num.strata.lambda,
  lambda.scale,
  parallel.flag,
  num.cores,
  bigmemory.flag,
  num.rows.chunk,
  col.names,
  type
)
}
\arguments{
  \item{formula}{Model formula describing the survival outcome and the set of
  predictors to include in the optimisation.}
  \item{data}{Input data set or connection to a big-memory backed design
  matrix that contains the variables referenced in \code{formula}.}
  \item{norm.method}{Normalization strategy applied to the feature matrix
  before optimisation, for example centring or standardising columns.}
  \item{features.mean}{Optional pre-computed column means used when
  normalising the features so that repeated fits can reuse shared
  statistics.}
  \item{features.sd}{Optional pre-computed column standard deviations used in
  concert with \code{features.mean} for scaling the predictors.}
  \item{opt.method}{Gradient based optimisation routine to employ, such as
  vanilla SGD or adaptive methods like Adam.}
  \item{beta.init}{Vector of starting values for the regression coefficients
  supplied when warm-starting the optimisation.}
  \item{beta.type}{Indicator controlling how \code{beta.init} is interpreted,
  for example whether the coefficients correspond to the original or
  normalised scale.}
  \item{lr.const}{Base learning-rate constant used by the stochastic
  gradient descent routine.}
  \item{lr.tau}{Learning-rate decay horizon or damping factor that moderates
  the step size schedule.}
  \item{strata.size}{Number of observations drawn per stratum when building
  mini-batches for the optimisation loop.}
  \item{batch.size}{Total number of observations assembled into each
  stochastic gradient batch.}
  \item{num.epoch}{Number of passes over the training data used during the
  optimisation.}
  \item{b1}{First exponential moving-average rate used by adaptive methods
  such as Adam to smooth gradients.}
  \item{b2}{Second exponential moving-average rate used by adaptive methods
  to smooth squared gradients.}
  \item{eps}{Numerical stabilisation constant added to denominators when
  updating the adaptive moments.}
  \item{inference.method}{Inference approach requested after fitting, for
  example naive asymptotics or bootstrap resampling.}
  \item{num.boot}{Number of bootstrap replicates to draw when
  \code{inference.method} relies on resampling.}
  \item{num.epoch.boot}{Number of optimisation epochs to run within each
  bootstrap replicate.}
  \item{boot.method}{Type of bootstrap scheme to apply, such as ordinary or
  stratified resampling.}
  \item{lr.const.boot}{Learning-rate constant used during bootstrap refits.}
  \item{lr.tau.boot}{Learning-rate decay factor applied during bootstrap
  refits.}
  \item{num.sample.strata}{Number of strata sampled without replacement during
  each bootstrap iteration when stratified resampling is selected.}
  \item{sig.level}{Significance level used when constructing confidence
  intervals or hypothesis tests.}
  \item{beta0}{Optional vector of coefficients under the null hypothesis when
  performing hypothesis tests.}
  \item{alpha}{Elastic-net mixing parameter controlling the relative weight of
  \eqn{\ell_1} and \eqn{\ell_2} regularisation penalties.}
  \item{lambda}{Sequence of regularisation strengths supplied explicitly for
  penalised estimation.}
  \item{nlambda}{Number of automatically generated \code{lambda} values when a
  grid is produced internally.}
  \item{num.strata.lambda}{Number of strata used when tuning \code{lambda} via
  cross-validation or other search procedures.}
  \item{lambda.scale}{Scale on which the \code{lambda} grid is generated, for
  example logarithmic or linear spacing.}
  \item{parallel.flag}{Logical flag enabling parallel computation of
  gradients or bootstrap replicates.}
  \item{num.cores}{Number of processing cores to use when parallel execution
  is enabled.}
  \item{bigmemory.flag}{Logical flag indicating whether intermediate matrices
  should be stored using \pkg{bigmemory} backed objects.}
  \item{num.rows.chunk}{Row chunk size to use when streaming data from an
  on-disk matrix representation.}
  \item{col.names}{Optional character vector of column names associated with
  the feature matrix.}
  \item{type}{Type of survival model to fit, for example Cox proportional
  hazards or accelerated failure time variants.}
}
\value{
A fitted model object storing the learned coefficients, optimisation
metadata, and any requested inference summaries.
}
\seealso{
\code{\link{bigscale}} for constructing normalised design matrices and
\code{\link{partialbigSurvSGDv0}} for partial fitting pipelines.
}
\examples{
\dontrun{
fit <- bigSurvSGD.na.omit(Surv(time, status) ~ ., data = my_training_data,
  norm.method = "standardize", opt.method = "adam", batch.size = 128,
  num.epoch = 10)
}
>>>>>>> theirs
}
